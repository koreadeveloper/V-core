"""
YouTube Video Summarizer with Groq Cloud
=========================================
A Streamlit application that extracts YouTube video transcripts or audio,
and uses Groq Cloud for AI-powered summarization.
"""

import streamlit as st
import os
import re
import uuid
import tempfile
from typing import Optional, Tuple, List
from datetime import datetime

# Groq and LangChain imports
from groq import Groq
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# YouTube imports
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
import yt_dlp

# Page configuration
st.set_page_config(
    page_title="YouTube AI ìš”ì•½ê¸° - Groq Cloud",
    page_icon="ğŸ¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #1e1e1e;
        border: 1px solid #333;
    }
    .summary-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 1rem;
        color: white;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2rem;
    }
    .stTabs [data-baseweb="tab"] {
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)


def extract_video_id(url: str) -> Optional[str]:
    """Extract YouTube video ID from various URL formats."""
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
        r'(?:embed\/)([0-9A-Za-z_-]{11})',
        r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None


def get_video_metadata(video_id: str) -> dict:
    """Get video metadata using yt-dlp."""
    try:
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': True,
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(f"https://www.youtube.com/watch?v={video_id}", download=False)
            return {
                'title': info.get('title', 'Unknown Title'),
                'thumbnail': info.get('thumbnail', ''),
                'duration': info.get('duration', 0),
                'channel': info.get('channel', 'Unknown Channel'),
                'view_count': info.get('view_count', 0),
            }
    except Exception as e:
        st.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            'title': 'Unknown Title',
            'thumbnail': '',
            'duration': 0,
            'channel': 'Unknown Channel',
            'view_count': 0,
        }


def get_transcript(video_id: str) -> Tuple[Optional[str], str]:
    """
    Step A: Try to get existing transcript from YouTube.
    Returns (transcript_text, source) where source is 'subtitle' or 'none'.
    Uses youtube-transcript-api v1.x API.
    """
    try:
        # Create API instance
        ytt_api = YouTubeTranscriptApi()
        
        # Try to fetch with Korean first, then English
        languages_to_try = [
            ['ko'],           # Korean
            ['en'],           # English
            ['ko', 'en'],     # Either
        ]
        
        transcript_data = None
        
        for langs in languages_to_try:
            try:
                fetched = ytt_api.fetch(video_id, languages=langs)
                transcript_data = fetched.to_raw_data()
                break
            except Exception:
                continue
        
        # If no transcript found with preferred languages, try to list and get any available
        if transcript_data is None:
            try:
                transcript_list = ytt_api.list(video_id)
                for transcript in transcript_list:
                    fetched = transcript.fetch()
                    transcript_data = fetched.to_raw_data()
                    break
            except Exception:
                pass
        
        if transcript_data:
            full_text = " ".join([entry['text'] for entry in transcript_data])
            return full_text, 'subtitle'
            
    except TranscriptsDisabled:
        st.info("ì´ ì˜ìƒì€ ìë§‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    except NoTranscriptFound:
        st.info("ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    except Exception as e:
        st.warning(f"ìë§‰ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return None, 'none'


def download_audio(video_id: str, output_dir: str) -> Optional[str]:
    """
    Step B: Download audio from YouTube using yt-dlp.
    Returns path to downloaded audio file.
    """
    unique_id = str(uuid.uuid4())[:8]
    output_path = os.path.join(output_dir, f"audio_{video_id}_{unique_id}")
    
    ydl_opts = {
        'format': 'bestaudio[ext=m4a]/bestaudio/best',
        'outtmpl': output_path + '.%(ext)s',
        'quiet': True,
        'no_warnings': True,
        'extract_audio': True,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '128',
        }],
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([f"https://www.youtube.com/watch?v={video_id}"])
        
        # Find the downloaded file
        for ext in ['mp3', 'm4a', 'webm', 'opus']:
            file_path = output_path + '.' + ext
            if os.path.exists(file_path):
                return file_path
                
        # Check if any file was created with the prefix
        for f in os.listdir(output_dir):
            if f.startswith(f"audio_{video_id}_{unique_id}"):
                return os.path.join(output_dir, f)
                
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return None


def transcribe_audio_with_groq(audio_path: str, api_key: str) -> Optional[str]:
    """
    Step 2: Use Groq Whisper API for speech-to-text.
    """
    try:
        client = Groq(api_key=api_key)
        
        # Check file size (Groq has limits)
        file_size = os.path.getsize(audio_path)
        if file_size > 25 * 1024 * 1024:  # 25MB limit
            st.warning("ì˜¤ë””ì˜¤ íŒŒì¼ì´ 25MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì²˜ìŒ 25MBë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        with open(audio_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                file=(os.path.basename(audio_path), audio_file.read()),
                model="whisper-large-v3",
                language="ko",  # Try Korean first
                response_format="text"
            )
        
        return transcription
        
    except Exception as e:
        st.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return None


def chunk_text(text: str, chunk_size: int = 4000, overlap: int = 200) -> List[str]:
    """
    Step 3: Split text into manageable chunks for LLM processing.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    chunks = splitter.split_text(text)
    return chunks


def summarize_chunk(chunk: str, chunk_num: int, total_chunks: int, llm: ChatGroq) -> str:
    """Map step: Summarize a single chunk."""
    map_prompt = PromptTemplate(
        input_variables=["chunk", "chunk_num", "total_chunks"],
        template="""ë‹¤ìŒì€ YouTube ì˜ìƒì˜ {chunk_num}/{total_chunks} ë¶€ë¶„ì…ë‹ˆë‹¤.

ì´ ë¶€ë¶„ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

{chunk}

ìš”ì•½:"""
    )
    
    chain = map_prompt | llm | StrOutputParser()
    result = chain.invoke({"chunk": chunk, "chunk_num": chunk_num, "total_chunks": total_chunks})
    return result


def final_summarize(summaries: List[str], llm: ChatGroq) -> dict:
    """Reduce step: Combine all chunk summaries into final output."""
    combined = "\n\n---\n\n".join(summaries)
    
    reduce_prompt = PromptTemplate(
        input_variables=["summaries"],
        template="""ë‹¤ìŒì€ YouTube ì˜ìƒì˜ ê° ë¶€ë¶„ë³„ ìš”ì•½ì…ë‹ˆë‹¤:

{summaries}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ¯ í•µì‹¬ ë‚´ìš© 3ì¤„ ìš”ì•½
(ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš© 3ê°€ì§€ë¥¼ ê°„ê²°í•˜ê²Œ)

## ğŸ“‹ íƒ€ì„ë¼ì¸ë³„ ìƒì„¸ ë‚´ìš©
(ì˜ìƒì˜ ì£¼ìš” íë¦„ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì„¤ëª…)

## ğŸ·ï¸ ì£¼ìš” í‚¤ì›Œë“œ í•´ì‹œíƒœê·¸
(ì˜ìƒì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´)

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
(ì˜ìƒì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ì£¼ìš” í†µì°°ì´ë‚˜ êµí›ˆ)

ë¶„ì„ ê²°ê³¼:"""
    )
    
    chain = reduce_prompt | llm | StrOutputParser()
    result = chain.invoke({"summaries": combined})
    
    return {
        'full_analysis': result,
        'chunk_summaries': summaries
    }


def process_video(video_id: str, api_key: str) -> dict:
    """Main processing pipeline."""
    results = {
        'metadata': None,
        'transcript': None,
        'source': None,
        'summary': None,
        'error': None
    }
    
    # Get metadata
    with st.status("ğŸ“Š ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...", expanded=True) as status:
        results['metadata'] = get_video_metadata(video_id)
        st.write(f"âœ… ì˜ìƒ ì œëª©: {results['metadata']['title']}")
        status.update(label="âœ… ì˜ìƒ ì •ë³´ ì™„ë£Œ", state="complete")
    
    # Step 1: Try to get transcript
    with st.status("ğŸ“ ìë§‰ í™•ì¸ ì¤‘...", expanded=True) as status:
        transcript, source = get_transcript(video_id)
        
        if transcript:
            results['transcript'] = transcript
            results['source'] = 'subtitle'
            st.write(f"âœ… ìë§‰ ì¶”ì¶œ ì™„ë£Œ ({len(transcript):,}ì)")
            status.update(label="âœ… ìë§‰ ì¶”ì¶œ ì™„ë£Œ", state="complete")
        else:
            status.update(label="âš ï¸ ìë§‰ ì—†ìŒ - ì˜¤ë””ì˜¤ ë¶„ì„ í•„ìš”", state="complete")
    
    # Step 2: If no transcript, download and transcribe audio
    if results['transcript'] is None:
        with st.status("ğŸµ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...", expanded=True) as status:
            with tempfile.TemporaryDirectory() as temp_dir:
                audio_path = download_audio(video_id, temp_dir)
                
                if audio_path:
                    st.write(f"âœ… ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    status.update(label="âœ… ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ", state="complete")
                    
                    with st.status("ğŸ¤ Groq Whisperë¡œ ìŒì„± ì¸ì‹ ì¤‘...", expanded=True) as stt_status:
                        transcript = transcribe_audio_with_groq(audio_path, api_key)
                        
                        if transcript:
                            results['transcript'] = transcript
                            results['source'] = 'whisper'
                            st.write(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ ({len(transcript):,}ì)")
                            stt_status.update(label="âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ", state="complete")
                        else:
                            results['error'] = "ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            stt_status.update(label="âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨", state="error")
                            return results
                else:
                    results['error'] = "ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    status.update(label="âŒ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨", state="error")
                    return results
    
    # Step 3 & 4: Chunk and summarize
    if results['transcript']:
        with st.status("ğŸ¤– AI ë¶„ì„ ì¤‘...", expanded=True) as status:
            # Initialize LLM
            llm = ChatGroq(
                groq_api_key=api_key,
                model_name="llama-3.3-70b-versatile",
                temperature=0.3,
                max_tokens=4096
            )
            
            # Chunk the text
            chunks = chunk_text(results['transcript'])
            st.write(f"ğŸ“„ í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            if len(chunks) == 1:
                # Short video - direct summarization
                st.write("ğŸ”„ ë‹¨ì¼ ìš”ì•½ ì§„í–‰ ì¤‘...")
                summary_result = final_summarize([results['transcript']], llm)
            else:
                # Long video - Map-Reduce
                st.write("ğŸ”„ Map-Reduce ìš”ì•½ ì§„í–‰ ì¤‘...")
                chunk_summaries = []
                
                progress_bar = st.progress(0)
                for i, chunk in enumerate(chunks):
                    chunk_summary = summarize_chunk(chunk, i+1, len(chunks), llm)
                    chunk_summaries.append(chunk_summary)
                    progress_bar.progress((i + 1) / len(chunks))
                
                summary_result = final_summarize(chunk_summaries, llm)
            
            results['summary'] = summary_result
            st.write("âœ… AI ë¶„ì„ ì™„ë£Œ!")
            status.update(label="âœ… AI ë¶„ì„ ì™„ë£Œ", state="complete")
    
    return results


def main():
    # Header
    st.markdown('<h1 class="main-header">ğŸ¬ YouTube AI ìš”ì•½ê¸°</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; color: #888;">Groq Cloud ê¸°ë°˜ ì´ˆê³ ì† ì˜ìƒ ë¶„ì„</p>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        api_key = st.text_input(
            "Groq API Key",
            type="password",
            placeholder="gsk_...",
            help="Groq Cloudì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )
        
        if api_key:
            st.success("âœ… API Key ì„¤ì •ë¨")
        else:
            st.warning("âš ï¸ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        st.divider()
        
        st.markdown("""
        ### ğŸ“Œ ì‚¬ìš© ë°©ë²•
        1. Groq API Key ì…ë ¥
        2. YouTube URL ë¶™ì—¬ë„£ê¸°
        3. 'ë¶„ì„ ì‹œì‘' í´ë¦­
        4. ê²°ê³¼ í™•ì¸!
        
        ### ğŸš€ ì²˜ë¦¬ ìˆœì„œ
        1. ìë§‰ ì¶”ì¶œ ì‹œë„
        2. (ìë§‰ ì—†ìœ¼ë©´) ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        3. Whisperë¡œ ìŒì„± ì¸ì‹
        4. LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
        """)
    
    # Main content
    col1, col2 = st.columns([3, 1])
    
    with col1:
        url = st.text_input(
            "YouTube URL",
            placeholder="https://www.youtube.com/watch?v=...",
            help="ë¶„ì„í•  YouTube ì˜ìƒì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”."
        )
    
    with col2:
        st.write("")  # Spacing
        st.write("")  # Spacing
        analyze_button = st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)
    
    # Process video
    if analyze_button:
        if not api_key:
            st.error("âŒ Groq API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not url:
            st.error("âŒ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        video_id = extract_video_id(url)
        if not video_id:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤.")
            return
        
        # Process the video
        results = process_video(video_id, api_key)
        
        if results['error']:
            st.error(f"âŒ ì˜¤ë¥˜: {results['error']}")
            return
        
        # Display results
        st.divider()
        
        # Video info header
        if results['metadata']:
            col1, col2 = st.columns([1, 2])
            with col1:
                if results['metadata']['thumbnail']:
                    st.image(results['metadata']['thumbnail'], use_container_width=True)
            with col2:
                st.subheader(results['metadata']['title'])
                st.caption(f"ğŸ“º {results['metadata']['channel']}")
                
                duration_mins = results['metadata']['duration'] // 60
                duration_secs = results['metadata']['duration'] % 60
                source_label = "ğŸ“ ìë§‰" if results['source'] == 'subtitle' else "ğŸ¤ Whisper STT"
                
                st.markdown(f"""
                - â±ï¸ **ê¸¸ì´**: {duration_mins}ë¶„ {duration_secs}ì´ˆ
                - ğŸ‘ï¸ **ì¡°íšŒìˆ˜**: {results['metadata']['view_count']:,}
                - ğŸ“„ **ì†ŒìŠ¤**: {source_label}
                - ğŸ“Š **í…ìŠ¤íŠ¸ ê¸¸ì´**: {len(results['transcript']):,}ì
                """)
        
        # Tabs for results
        if results['summary']:
            tab1, tab2, tab3 = st.tabs(["ğŸ“‹ ìš”ì•½", "ğŸ“Š ìƒì„¸ ë¶„ì„", "ğŸ“„ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸"])
            
            with tab1:
                st.markdown(results['summary']['full_analysis'])
            
            with tab2:
                if len(results['summary']['chunk_summaries']) > 1:
                    st.subheader("ì²­í¬ë³„ ìš”ì•½")
                    for i, chunk_summary in enumerate(results['summary']['chunk_summaries']):
                        with st.expander(f"ğŸ“„ Part {i+1}"):
                            st.write(chunk_summary)
                else:
                    st.info("ì´ ì˜ìƒì€ ì§§ì•„ì„œ ë‹¨ì¼ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            with tab3:
                st.text_area(
                    "ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸",
                    value=results['transcript'],
                    height=400,
                    disabled=True
                )
                
                # Download button
                st.download_button(
                    label="ğŸ“¥ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ",
                    data=results['transcript'],
                    file_name=f"transcript_{video_id}.txt",
                    mime="text/plain"
                )


if __name__ == "__main__":
    main()
